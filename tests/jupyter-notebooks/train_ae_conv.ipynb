{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "\n",
    "from utils import plot_red_comp, slicer, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoeconder\n",
    "This code trains a simple autoencoder neural network using convolutional\n",
    " layers.The contraction and expansion of the implemented neural network used\n",
    "only convolutional layers. Therefore, it does not rely on maxpooling or\n",
    "upsampling layers. Instead, it was used strides to control the contraction\n",
    "and expansion of the neural network. Also, in the decoder part it was used a\n",
    "decovolutional process.\n",
    "\n",
    "For the latent space it was used a fully connected layer with an additional\n",
    "fully connected layer in sequence, to connect the latent space with the\n",
    "decoder convolutional layer.\n",
    "\n",
    "The neural network architecture with the activation function is stated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting data\n",
    "dt_fl = \"nn_data.h5\"\n",
    "dt_dst = \"scaled_data\"\n",
    "\n",
    "# The percentage for the test is implicit\n",
    "n_train = 0.8\n",
    "n_valid = 0.1\n",
    "\n",
    "# Select the variable to train\n",
    "# 0: Temperature - 1: Pressure - 2: Velocity - None: all\n",
    "var = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Open data file\n",
    "f = h5py.File(dt_fl, \"r\")\n",
    "dt = f[dt_dst]\n",
    "\n",
    "# Split data file\n",
    "idxs = split(dt.shape[0], n_train, n_valid)\n",
    "slc_trn, slc_vld, slc_tst = slicer(dt.shape, idxs, var=var)\n",
    "# Slice data\n",
    "x_train = dt[slc_trn][:, :, :, np.newaxis]\n",
    "x_val = dt[slc_vld][:, :, :, np.newaxis]\n",
    "\n",
    "# Convert the var into a slice\n",
    "if var:\n",
    "    slc = slice(var, var + 1)\n",
    "else:\n",
    "    slc = slice(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder settings\n",
    "\n",
    "# Activation function\n",
    "act = \"tanh\"  # Convolutional layers activation function\n",
    "act_lt = \"tanh\"  # Latent space layers activation function\n",
    "# Number of filters of each layer\n",
    "flt = [3, 9, 27]\n",
    "# Filter size\n",
    "flt_size = 5\n",
    "# Strides of each layer\n",
    "strd = [2, 2, 5]\n",
    "# Latent space size\n",
    "lt_sz = 50\n",
    "\n",
    "# Training settings\n",
    "opt = \"adam\"  # Optimizer\n",
    "loss = \"mse\"\n",
    "epochs = 60\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder neural network\n",
    "tf.keras.backend.clear_session()\n",
    "flt_tp = (flt_size, flt_size)\n",
    "conv_kwargs = dict(activation=act, padding=\"same\")\n",
    "# Encoder\n",
    "inputs = layers.Input(shape=x_train.shape[1:])\n",
    "e = layers.Conv2D(flt[0], flt_tp, strides=strd[0], **conv_kwargs)(inputs)\n",
    "e = layers.Conv2D(flt[1], flt_tp, strides=strd[1], **conv_kwargs)(e)\n",
    "e = layers.Conv2D(flt[2], flt_tp, strides=strd[2], **conv_kwargs)(e)\n",
    "\n",
    "# Latent space\n",
    "l = layers.Flatten()(e)\n",
    "l = layers.Dense(lt_sz, activation=act_lt)(l)\n",
    "\n",
    "# Latent to decoder\n",
    "dn_flt = flt[-1]\n",
    "d_shp = (x_train.shape[1:-1] / np.prod(strd)).astype(int)\n",
    "d_sz = np.prod(d_shp) * dn_flt\n",
    "d = layers.Dense(d_sz, activation=act_lt)(l)\n",
    "d = layers.Reshape(np.hstack((d_shp, dn_flt)))(d)\n",
    "# Decoder\n",
    "d = layers.Conv2DTranspose(flt[-1], flt_tp, strides=strd[-1], **conv_kwargs)(d)\n",
    "d = layers.Conv2DTranspose(flt[-2], flt_tp, strides=strd[-2], **conv_kwargs)(d)\n",
    "d = layers.Conv2DTranspose(flt[-3], flt_tp, strides=strd[-3], **conv_kwargs)(d)\n",
    "decoded = layers.Conv2DTranspose(\n",
    "    x_train.shape[-1], flt_tp, activation=\"linear\", padding=\"same\"\n",
    ")(d)\n",
    "\n",
    "# Mount the autoencoder\n",
    "ae = Model(inputs, decoded, name=\"Convolutional Autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the architecture\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "Early stopping to stop training when the validation loss start to increase\n",
    "The patience term is a number of epochs to wait before stop. Also, the\n",
    "'restore_best_weights' is used to restore the best model against the\n",
    "validation dataset. It is necessary as not always the best model against\n",
    "the validation dataset is the last neural network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "monitor = \"val_loss\"\n",
    "patience = int(epochs * 0.3)\n",
    "es = EarlyStopping(\n",
    "    monitor=monitor, mode=\"min\", patience=patience, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "ae.compile(optimizer=opt, loss=loss)\n",
    "hist = ae.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_val, x_val),\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the history to a Pandas dataframe\n",
    "hist = pd.DataFrame(hist.history)\n",
    "hist.index.name = \"Epochs\"\n",
    "\n",
    "# Plot training evolution\n",
    "tit = \"Validation loss: {:.3f} - Training loss: {:.3f}\".format(*hist.min())\n",
    "hist.plot(grid=True, title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained neural network against the test dataset\n",
    "x_test = dt[slc_tst][:, :, :, np.newaxis]\n",
    "loss = ae.evaluate(x_test, x_test)\n",
    "print(\"Test dataset loss: {:.3f}\".format(loss))\n",
    "\n",
    "global_loss = ae.evaluate(dt[:, :, :, slc], dt[:, :, :, slc])\n",
    "print(\"Entire dataset loss: {:.3f}\".format(global_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the input and output of the autoencoder neural network\n",
    "data_index = 634\n",
    "\n",
    "# Slice the data\n",
    "dt_in = dt[data_index, :, :, slc]\n",
    "# Get the neural network output\n",
    "dt_out = ae.predict(dt_in[np.newaxis])\n",
    "# Plot\n",
    "alg = \"Convolutional Autoencoder\"\n",
    "plot_red_comp(dt_in, dt_out[0], 0, lt_sz, global_loss, alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
