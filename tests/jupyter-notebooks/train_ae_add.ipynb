{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "\n",
    "from utils import plot_red_comp, slicer, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE-2 Autoeconder\n",
    "This code trains a neural network based on the VQ-VAE-2 autoencoder available\n",
    "at [ArXiv](https://arxiv.org/pdf/1906.00446.pdf). This autoencoder was\n",
    "developed to generate diverse high-fidelity images that competes with the\n",
    "state of the art algorithms, generative adversarial networks (GAN), in\n",
    "problems to generete images from a probability distribution.\n",
    "\n",
    "The main characteristic from the VQ-VAE-2 architecture is the use of vector\n",
    "quantization and the use of multiple latent spaces. In the vector quantization,\n",
    "the latent space is quantized into a codebook of a given size. In the multiple\n",
    "latent one is obtained from a less compressed state and the other from a\n",
    "further compressed state. Then, each latent space is decoded and they are\n",
    "added when their shapes are equal.\n",
    "\n",
    "The vector quantization is particularly interesting for image generation\n",
    "as it can be used for density estimation. Thus, it can be randomly sampled\n",
    "and then generate random images. As this property has limited use for \n",
    "surrogate models. This implementation only used the multi latent spaces \n",
    "proposed by the paper.\n",
    "\n",
    "The contraction and expansion of the implemented neural network used\n",
    "only convolutional layers. Therefore, it does not rely on maxpooling or\n",
    "upsampling layers. Instead, it was used strides to control the contraction\n",
    "and expansion of the neural network. Also, in the decoder part it was used a\n",
    "decovolutional process.\n",
    "\n",
    "For the latent space it was used a fully connected layer with an additional\n",
    "fully connected layer in sequence, to connect the latent space with the\n",
    "decoder convolutional layer.\n",
    "\n",
    "The neural network architecture with the activation function is stated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting data\n",
    "dt_fl = \"nn_data.h5\"\n",
    "dt_dst = \"scaled_data\"\n",
    "\n",
    "# The percentage for the test is implicit\n",
    "n_train = 0.8\n",
    "n_valid = 0.1\n",
    "\n",
    "# Select the variable to train\n",
    "# 0: Temperature - 1: Pressure - 2: Velocity - None: all\n",
    "var = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Open data file\n",
    "f = h5py.File(dt_fl, \"r\")\n",
    "dt = f[dt_dst]\n",
    "\n",
    "# Split data file\n",
    "idxs = split(dt.shape[0], n_train, n_valid)\n",
    "slc_trn, slc_vld, slc_tst = slicer(dt.shape, idxs, var=var)\n",
    "# Slice data\n",
    "x_train = dt[slc_trn][:, :, :, np.newaxis]\n",
    "x_val = dt[slc_vld][:, :, :, np.newaxis]\n",
    "\n",
    "# Convert the var into a slice\n",
    "if var:\n",
    "    slc = slice(var, var + 1)\n",
    "else:\n",
    "    slc = slice(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder settings\n",
    "\n",
    "# Activation function\n",
    "act = \"tanh\"  # Convolutional layers activation function\n",
    "act_lt = \"tanh\"  # Latent space layers activation function\n",
    "# Number of filters of each layer\n",
    "flt = [3, 9, 27]\n",
    "# Filter size\n",
    "flt_size = 5\n",
    "# Strides of each layer\n",
    "strd = [2, 2, 5]\n",
    "# Latent space size\n",
    "lt_sz = [25, 25]\n",
    "\n",
    "# Training settings\n",
    "opt = \"adam\"  # Optimizer\n",
    "loss = \"mse\"\n",
    "epochs = 60\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder neural network\n",
    "tf.keras.backend.clear_session()\n",
    "flt_tp = (flt_size, flt_size)\n",
    "conv_kwargs = dict(activation=act, padding=\"same\")\n",
    "# Encoder\n",
    "inputs = layers.Input(shape=x_train.shape[1:])\n",
    "e = layers.Conv2D(flt[0], flt_tp, strides=strd[0], **conv_kwargs)(inputs)\n",
    "ed = layers.Conv2D(1, (1, 1), padding=\"same\")(e)\n",
    "e = layers.Conv2D(flt[1], flt_tp, strides=strd[1], **conv_kwargs)(e)\n",
    "e = layers.Conv2D(flt[2], flt_tp, strides=strd[2], **conv_kwargs)(e)\n",
    "\n",
    "# Latent space\n",
    "l1 = layers.Flatten()(e)\n",
    "l1 = layers.Dense(lt_sz[0], activation=act_lt)(l1)\n",
    "\n",
    "l2 = layers.Flatten()(ed)\n",
    "l2 = layers.Dense(lt_sz[1], activation=act_lt)(l2)\n",
    "\n",
    "# Latent to decoder\n",
    "dn_flt = flt[-1]\n",
    "d_shp = (x_train.shape[1:-1] / np.prod(strd)).astype(int)\n",
    "d_sz = np.prod(d_shp) * dn_flt\n",
    "d = layers.Dense(d_sz, activation=act_lt)(l1)\n",
    "d = layers.Reshape(np.hstack((d_shp, dn_flt)))(d)\n",
    "# Decoder\n",
    "d = layers.Conv2DTranspose(flt[-1], flt_tp, strides=strd[-1], **conv_kwargs)(d)\n",
    "d = layers.Conv2DTranspose(flt[-2], flt_tp, strides=strd[-2], **conv_kwargs)(d)\n",
    "# Add latent 2\n",
    "d1n_shp = (x_train.shape[1:-1] / np.array(strd[0])).astype(int)\n",
    "d2 = layers.Dense(np.prod(d1n_shp), activation=\"linear\")(l2)\n",
    "d2 = layers.Reshape(np.hstack([d1n_shp, [1]]))(d2)\n",
    "d2 = layers.Conv2D(flt[-2], (1, 1), padding=\"same\")(d2)\n",
    "d = layers.Add()([d2, d])\n",
    "# Back to decoder\n",
    "d = layers.Conv2DTranspose(flt[-3], flt_tp, strides=strd[-3], **conv_kwargs)(d)\n",
    "decoded = layers.Conv2DTranspose(\n",
    "    x_train.shape[-1], flt_tp, activation=\"linear\", padding=\"same\"\n",
    ")(d)\n",
    "\n",
    "# Mount the autoencoder\n",
    "ae = Model(inputs, decoded, name=\"Based on VQ-VAE 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the architecture\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "Early stopping to stop training when the validation loss start to increase\n",
    "The patience term is a number of epochs to wait before stop. Also, the\n",
    "'restore_best_weights' is used to restore the best model against the\n",
    "validation dataset. It is necessary as not always the best model against\n",
    "the validation dataset is the last neural network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "monitor = \"val_loss\"\n",
    "patience = int(epochs * 0.3)\n",
    "es = EarlyStopping(\n",
    "    monitor=monitor, mode=\"min\", patience=patience, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "ae.compile(optimizer=opt, loss=loss)\n",
    "hist = ae.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_val, x_val),\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the history to a Pandas dataframe\n",
    "hist = pd.DataFrame(hist.history)\n",
    "hist.index.name = \"Epochs\"\n",
    "\n",
    "# Plot training evolution\n",
    "tit = \"Validation loss: {:.3f} - Training loss: {:.3f}\".format(*hist.min())\n",
    "hist.plot(grid=True, title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained neural network against the test dataset\n",
    "x_test = dt[slc_tst][:, :, :, np.newaxis]\n",
    "loss = ae.evaluate(x_test, x_test)\n",
    "print(\"Test dataset loss: {:.3f}\".format(loss))\n",
    "\n",
    "global_loss = ae.evaluate(dt[:, :, :, slc], dt[:, :, :, slc])\n",
    "print(\"Entire dataset loss: {:.3f}\".format(global_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the input and output of the autoencoder neural network\n",
    "data_index = 634\n",
    "\n",
    "# Slice the data\n",
    "dt_in = dt[data_index, :, :, slc]\n",
    "# Get the neural network output\n",
    "dt_out = ae.predict(dt_in[np.newaxis])\n",
    "# Plot\n",
    "alg = \"VQ-VAE-2 based autoencoder\"\n",
    "plot_red_comp(dt_in, dt_out[0], 0, lt_sz, global_loss, alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
